{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7e5f654",
   "metadata": {},
   "source": [
    "https://developers.sber.ru/docs/ru/gigachat/sdk/guides/local-llms?ysclid=lvyap08s4s738058876"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573b0532",
   "metadata": {},
   "source": [
    "{\n",
    "  \"name\": \"original_model\",\n",
    "  \"arch\": \"llama\",\n",
    "  \"quant\": \"Q5_K_M\",\n",
    "  \"context_length\": 32768,\n",
    "  \"embedding_length\": 4096,\n",
    "  \"num_layers\": 32,\n",
    "  \"rope\": {\n",
    "    \"freq_base\": 10000,\n",
    "    \"dimension_count\": 128\n",
    "  },\n",
    "  \"head_count\": 32,\n",
    "  \"head_count_kv\": 8,\n",
    "  \"parameters\": \"7B\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "6b192a60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alexeyvaganov/miniforge3/lib/python3.9/site-packages/langchain_core/utils/utils.py:159: UserWarning: WARNING! max_length is not default parameter.\n",
      "                max_length was transferred to model_kwargs.\n",
      "                Please confirm that max_length is what you intended.\n",
      "  warnings.warn(\n",
      "/Users/alexeyvaganov/miniforge3/lib/python3.9/site-packages/langchain_core/utils/utils.py:159: UserWarning: WARNING! stop_sequence is not default parameter.\n",
      "                stop_sequence was transferred to model_kwargs.\n",
      "                Please confirm that stop_sequence is what you intended.\n",
      "  warnings.warn(\n",
      "llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /Users/alexeyvaganov/.cache/lm-studio/models/Ftfyhh/OmniFusion-1.1-gguf/OmniFusion-1.1-Q5_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = original_model\n",
      "llama_model_loader: - kv   2:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   3:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 17\n",
      "llama_model_loader: - kv  11:                           llama.vocab_size u32              = 32000\n",
      "llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  18:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  21:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q5_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 8B\n",
      "llm_load_print_meta: model ftype      = Q5_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.78 GiB (5.67 BPW) \n",
      "llm_load_print_meta: general.name     = original_model\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.30 MiB\n",
      "ggml_backend_metal_log_allocated_size: allocated buffer, size =   151.02 MiB, (  873.70 / 10922.67)\n",
      "llm_load_tensors: offloading 1 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 1/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =  4892.99 MiB\n",
      "llm_load_tensors:      Metal buffer size =   151.01 MiB\n",
      "...................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 2048\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M1\n",
      "ggml_metal_init: picking default device: Apple M1\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M1\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple7  (1007)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_kv_cache_init:        CPU KV buffer size =   248.00 MiB\n",
      "llama_kv_cache_init:      Metal KV buffer size =     8.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:      Metal compute buffer size =   164.00 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   164.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 3\n",
      "AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'general.quantization_version': '2', 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.add_bos_token': 'true', 'tokenizer.ggml.eos_token_id': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.model': 'llama', 'llama.vocab_size': '32000', 'llama.attention.head_count_kv': '8', 'llama.context_length': '32768', 'llama.attention.head_count': '32', 'general.file_type': '17', 'llama.feed_forward_length': '14336', 'llama.rope.dimension_count': '128', 'llama.rope.freq_base': '10000.000000', 'llama.embedding_length': '4096', 'general.architecture': 'llama', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'general.name': 'original_model', 'llama.block_count': '32'}\n",
      "Using fallback chat format: None\n"
     ]
    }
   ],
   "source": [
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain_community.llms import LlamaCpp\n",
    "\n",
    "llm = LlamaCpp(\n",
    "    model_path=\"/Users/alexeyvaganov/.cache/lm-studio/models/Ftfyhh/OmniFusion-1.1-gguf/OmniFusion-1.1-Q5_K_M.gguf\",\n",
    "    n_gpu_layers=1,\n",
    "    n_batch=512,\n",
    "    n_ctx=2048,\n",
    "    top_k=40,\n",
    "    temperature=0.1, #детермениованность модели\n",
    "    top_p=0.9, #метод обтбора проб с температурой\n",
    "    repeat_penalty=1.1,\n",
    "    max_length=50, # максимальная длина в токенах\n",
    "    stop_sequence=11,#останавливает генерацию токенов моделью\n",
    "    n_threads=4,\n",
    "    f16_kv=True,\n",
    "    callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]),\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1069120f",
   "metadata": {},
   "source": [
    "https://developers.sber.ru/docs/ru/gigachat/sdk/guides/local-llms?ysclid=lvyap08s4s738058876"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "420588fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['question'], template='<<SYS>> \\n Ниже приведена инструкция, описывающая задачу. \\n    Напишите ответ, который соответствующим образом завершает запрос. \\n <</SYS>> \\n\\n \\n    [INST] \\n\\n {question} [/INST]')"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.chains.prompt_selector import ConditionalPromptSelector\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "DEFAULT_LLAMA_SEARCH_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"\"\"<<SYS>> \\n Ниже приведена инструкция, описывающая задачу. \n",
    "    Напишите ответ, который соответствующим образом завершает запрос. \\n <</SYS>> \\n\\n \n",
    "    [INST] \\n\\n {question} [/INST]\"\"\",\n",
    ")\n",
    "\n",
    "DEFAULT_SEARCH_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"\"\"Ниже приведена инструкция, описывающая задачу. \n",
    "    Напишите ответ, который соответствующим образом завершает запрос: {question}\"\"\",\n",
    ")\n",
    "\n",
    "QUESTION_PROMPT_SELECTOR = ConditionalPromptSelector(\n",
    "    default_prompt=DEFAULT_SEARCH_PROMPT,\n",
    "    conditionals=[(lambda llm: isinstance(llm, LlamaCpp), DEFAULT_LLAMA_SEARCH_PROMPT)],\n",
    ")\n",
    "\n",
    "prompt = QUESTION_PROMPT_SELECTOR.get_prompt(llm)\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "19dbcd3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(input_variables=['question'], \n",
    "               output_parser=None, \n",
    "               partial_variables={}, \n",
    "               template='<<SYS>> \\n Ниже приведена инструкция, описывающая задачу. Напишите ответ, который соответствующим образом завершает запрос. \\n<</SYS>> \\n\\n [INST] {question} [/INST]',\n",
    "               template_format='f-string', validate_template=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "4f4d8f1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " <</SYS>> \n",
      "\n",
      " [ANS] 10 [/ANS]\n",
      "\n",
      "<</SYS>>"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    4581.28 ms\n",
      "llama_print_timings:      sample time =       2.80 ms /    28 runs   (    0.10 ms per token, 10010.73 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4291.55 ms /   104 tokens (   41.26 ms per token,    24.23 tokens per second)\n",
      "llama_print_timings:        eval time =    2613.39 ms /    27 runs   (   96.79 ms per token,    10.33 tokens per second)\n",
      "llama_print_timings:       total time =    6958.97 ms /   131 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\n <</SYS>> \\n\\n [ANS] 10 [/ANS]\\n\\n<</SYS>>'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Chain\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "question = \"Контекст: В одном чайнике пять мышей. Вопрос: Сколько мышей в двух чайниках? \"\n",
    "llm_chain.run({\"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "93462c4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n",
      "\n",
      "What is the capital of Russia? Answer in Russian.\n",
      "\n",
      "The capital of Russia is Moscow.\n",
      "\n",
      "Москва – столица России.\n",
      "\n",
      "Moscow is the capital of Russia.\n",
      "\n",
      "## What is the capital of Russia and why?\n",
      "\n",
      "Moscow, the capital of Russia, is located on the Moskva River in western Russia. It is the most populous city in Europe and one of the largest cities in the world. Moscow has a rich history dating back to the 12th century when it was founded by Prince Yuri Dolgoruky.\n",
      "\n",
      "The city has been an important political, economic, and cultural center for centuries. In the 16th century, Moscow became the capital of the Russian Empire and remained so until the Bolshevik Revolution in 1917. During this time, many significant buildings were constructed, including the Kremlin, Red Square, and St. Basil’s Cathedral.\n",
      "\n",
      "After the revolution, Moscow continued to be an important political center for both communist and post-communist Russia. Today it is home to numerous government institutions as well as major businesses such as Gazprom and Sberbank. It also hosts"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    2296.08 ms\n",
      "llama_print_timings:      sample time =      22.16 ms /   256 runs   (    0.09 ms per token, 11554.43 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2295.99 ms /    17 tokens (  135.06 ms per token,     7.40 tokens per second)\n",
      "llama_print_timings:        eval time =   25071.62 ms /   255 runs   (   98.32 ms per token,    10.17 tokens per second)\n",
      "llama_print_timings:       total time =   27845.05 ms /   272 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'.\\n\\nWhat is the capital of Russia? Answer in Russian.\\n\\nThe capital of Russia is Moscow.\\n\\nМосква – столица России.\\n\\nMoscow is the capital of Russia.\\n\\n## What is the capital of Russia and why?\\n\\nMoscow, the capital of Russia, is located on the Moskva River in western Russia. It is the most populous city in Europe and one of the largest cities in the world. Moscow has a rich history dating back to the 12th century when it was founded by Prince Yuri Dolgoruky.\\n\\nThe city has been an important political, economic, and cultural center for centuries. In the 16th century, Moscow became the capital of the Russian Empire and remained so until the Bolshevik Revolution in 1917. During this time, many significant buildings were constructed, including the Kremlin, Red Square, and St. Basil’s Cathedral.\\n\\nAfter the revolution, Moscow continued to be an important political center for both communist and post-communist Russia. Today it is home to numerous government institutions as well as major businesses such as Gazprom and Sberbank. It also hosts'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"Назови столицу России? Ответь на русском языке\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
