{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "07ee4424-0d13-4ebb-aba1-3755153dfcc1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'OpenAI' from 'openai' (/Users/alexeyvaganov/miniforge3/lib/python3.9/site-packages/openai/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Example: reuse your existing OpenAI setup\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mopenai\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OpenAI\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'OpenAI' from 'openai' (/Users/alexeyvaganov/miniforge3/lib/python3.9/site-packages/openai/__init__.py)"
     ]
    }
   ],
   "source": [
    "# Example: reuse your existing OpenAI setup\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "03dbe564",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai._client import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "85c4f5a1-0574-4cf8-a561-0644d63da137",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Point to the local server\n",
    "client = OpenAI(base_url=\"http://localhost:1234/v1\", api_key=\"lm-studio\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "9cb7e6e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessage(content=\"\\nДля того, чтобы передать картинку, необходимо использовать метод `send_file` в библиотеке Flask. Например, если вы хотите отправить файл `image.jpg`, то можно написать следующий код:\\n```python\\nfrom flask import send_file\\n\\n@app.route('/send_image')\\ndef send_image():\\n    return send_file('image.jpg', mimetype='image/jpeg')\\n```\\nВ этом примере мы создаем маршрут `/send_image`, который отвечает на запрос GET и возвращает файл `image.jpg`. Мы также указываем тип MIME для файла, чтобы браузер знал, как его отобразить.\\n\\nЕсли вы хотите передавать картинку из базы данных или другого источника, то вам нужно будет получить путь к файлу и использовать метод `send_file` с этим путем. Например, если вы храните картинки в базе данных, то можно написать следующий код:\\n```python\\nfrom flask import send_file\\n\\n@app.route('/send_image')\\ndef send_image():\\n    image_path = 'path/to/image.jpg'  # Путь к картинке в базе данных\\n    return send_file(image_path, mimetype='image/jpeg')\\n```\\nВ этом примере мы получаем путь к файлу из базы данных и передаем его методу `send_file`.\", role='assistant', function_call=None, tool_calls=None)\n"
     ]
    }
   ],
   "source": [
    "#Если не знаешь ответ, ответь: Я не знаю. \n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "  model=\"Ftfyhh/OmniFusion-1.1-gguf\",\n",
    "  messages=[\n",
    "    {\"role\": \"user\", \n",
    "     \"content\": \"\"\"\"Отвечай на русском языке. \n",
    "     Вопрос: а как тебе передать картинку?\"\"\"}\n",
    "  ],\n",
    "  temperature=0.1,\n",
    ")\n",
    "print(completion.choices[0].message)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353b15fc",
   "metadata": {},
   "source": [
    "------------------------------------------------------\n",
    "Другой способ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eab07c7",
   "metadata": {},
   "source": [
    "https://github.com/abetlen/llama-cpp-python/blob/main/docs/install/macos.md\n",
    "https://python.langchain.com/docs/use_cases/question_answering/local_retrieval_qa/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "07f8c756",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Skipping llama-cpp-python as it is not installed.\u001b[0m\u001b[33m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip uninstall llama-cpp-python -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "7caf35e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting llama-cpp-python\n",
      "  Downloading llama_cpp_python-0.2.69.tar.gz (42.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.5/42.5 MB\u001b[0m \u001b[31m324.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:04\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting diskcache>=5.6.1\n",
      "  Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 KB\u001b[0m \u001b[31m504.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: jinja2>=2.11.3 in /Users/alexeyvaganov/miniforge3/lib/python3.9/site-packages (from llama-cpp-python) (3.1.2)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /Users/alexeyvaganov/miniforge3/lib/python3.9/site-packages (from llama-cpp-python) (4.11.0)\n",
      "Requirement already satisfied: numpy>=1.20.0 in /Users/alexeyvaganov/miniforge3/lib/python3.9/site-packages (from llama-cpp-python) (1.26.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/alexeyvaganov/miniforge3/lib/python3.9/site-packages (from jinja2>=2.11.3->llama-cpp-python) (2.1.1)\n",
      "Building wheels for collected packages: llama-cpp-python\n",
      "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.2.69-cp39-cp39-macosx_14_0_arm64.whl size=3388946 sha256=de53224486bf82c55d56b2ccac7a52aa6a056161257def7f658413aa9b58357b\n",
      "  Stored in directory: /private/var/folders/87/c02w9bp14k1fx5943h90k0500000gn/T/pip-ephem-wheel-cache-j09u_hla/wheels/08/35/9f/507006d7d467568d953dd224da2e4101240f387bc5c563a47b\n",
      "Successfully built llama-cpp-python\n",
      "Installing collected packages: diskcache, llama-cpp-python\n",
      "Successfully installed diskcache-5.6.3 llama-cpp-python-0.2.69\n"
     ]
    }
   ],
   "source": [
    "!CMAKE_ARGS=\"-DLLAMA_METAL=on\" pip install -U llama-cpp-python --no-cache-dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "bc5b2d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import LlamaCpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "72aa3de2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alexeyvaganov/miniforge3/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "from tensorflow. python. client import device_lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f7ca4e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_available_gpus():\n",
    "    local_device_protos = device_lib. list_local_devices()\n",
    "    return [x. name for x in local_device_protos if x. device_type == 'GPU']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "20abb5e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-06 20:46:39.528067: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-05-06 20:46:39.529063: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['/device:GPU:0']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_available_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "e5acc84d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 18 key-value pairs and 485 tensors from /Users/alexeyvaganov/.cache/lm-studio/models/DFofanov78/ruGPT-3.5-13B-GGUF/ruGPT-3.5-13B-Q4_0.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = gpt2\n",
      "llama_model_loader: - kv   1:                               general.name str              = ruGPT-3.5-13B\n",
      "llama_model_loader: - kv   2:                           gpt2.block_count u32              = 40\n",
      "llama_model_loader: - kv   3:                        gpt2.context_length u32              = 2048\n",
      "llama_model_loader: - kv   4:                      gpt2.embedding_length u32              = 5120\n",
      "llama_model_loader: - kv   5:                   gpt2.feed_forward_length u32              = 20480\n",
      "llama_model_loader: - kv   6:                  gpt2.attention.head_count u32              = 40\n",
      "llama_model_loader: - kv   7:          gpt2.attention.layer_norm_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv   8:                          general.file_type u32              = 2\n",
      "llama_model_loader: - kv   9:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  10:                      tokenizer.ggml.tokens arr[str,50272]   = [\"<pad>\", \"<|endoftext|>\", \"<s>\", \"</...\n",
      "llama_model_loader: - kv  11:                  tokenizer.ggml.token_type arr[i32,50272]   = [3, 3, 3, 3, 1, 3, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.merges arr[str,49995]   = [\"Ġ Ð\", \"Ð ¾\", \"Ð µ\", \"Ð °\", ...\n",
      "llama_model_loader: - kv  13:                tokenizer.ggml.bos_token_id u32              = 2\n",
      "llama_model_loader: - kv  14:                tokenizer.ggml.eos_token_id u32              = 3\n",
      "llama_model_loader: - kv  15:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  16:               tokenizer.ggml.add_bos_token bool             = false\n",
      "llama_model_loader: - kv  17:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:  322 tensors\n",
      "llama_model_loader: - type q4_0:  162 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: missing pre-tokenizer type, using: 'default'\n",
      "llm_load_vocab:                                             \n",
      "llm_load_vocab: ************************************        \n",
      "llm_load_vocab: GENERATION QUALITY WILL BE DEGRADED!        \n",
      "llm_load_vocab: CONSIDER REGENERATING THE MODEL             \n",
      "llm_load_vocab: ************************************        \n",
      "llm_load_vocab:                                             \n",
      "llm_load_vocab: mismatch in special tokens definition ( 21/50272 vs 20/50272 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = gpt2\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 50272\n",
      "llm_load_print_meta: n_merges         = 49995\n",
      "llm_load_print_meta: n_ctx_train      = 2048\n",
      "llm_load_print_meta: n_embd           = 5120\n",
      "llm_load_print_meta: n_head           = 40\n",
      "llm_load_print_meta: n_head_kv        = 40\n",
      "llm_load_print_meta: n_layer          = 40\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 5120\n",
      "llm_load_print_meta: n_embd_v_gqa     = 5120\n",
      "llm_load_print_meta: f_norm_eps       = 1.0e-05\n",
      "llm_load_print_meta: f_norm_rms_eps   = 0.0e+00\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 20480\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = -1\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 2048\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = ?B\n",
      "llm_load_print_meta: model ftype      = Q4_0\n",
      "llm_load_print_meta: model params     = 13.11 B\n",
      "llm_load_print_meta: model size       = 6.94 GiB (4.55 BPW) \n",
      "llm_load_print_meta: general.name     = ruGPT-3.5-13B\n",
      "llm_load_print_meta: BOS token        = 2 '<s>'\n",
      "llm_load_print_meta: EOS token        = 3 '</s>'\n",
      "llm_load_print_meta: PAD token        = 0 '<pad>'\n",
      "llm_load_print_meta: LF token         = 134 'Ä'\n",
      "llm_load_tensors: ggml ctx size =    0.46 MiB\n",
      "ggml_backend_metal_log_allocated_size: allocated buffer, size =   169.02 MiB, (  494.67 / 10922.67)\n",
      "llm_load_tensors: offloading 1 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 1/41 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =  7105.26 MiB\n",
      "llm_load_tensors:      Metal buffer size =   169.01 MiB\n",
      "...................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 2048\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M1\n",
      "ggml_metal_init: picking default device: Apple M1\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M1\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple7  (1007)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_kv_cache_init:        CPU KV buffer size =  1560.00 MiB\n",
      "llama_kv_cache_init:      Metal KV buffer size =    40.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 1600.00 MiB, K (f16):  800.00 MiB, V (f16):  800.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB\n",
      "llama_new_context_with_model:      Metal compute buffer size =   184.00 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   184.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1489\n",
      "llama_new_context_with_model: graph splits = 3\n",
      "AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'general.quantization_version': '2', 'tokenizer.ggml.add_bos_token': 'false', 'tokenizer.ggml.eos_token_id': '3', 'gpt2.attention.layer_norm_epsilon': '0.000010', 'gpt2.block_count': '40', 'general.file_type': '2', 'gpt2.attention.head_count': '40', 'tokenizer.ggml.padding_token_id': '0', 'gpt2.feed_forward_length': '20480', 'tokenizer.ggml.model': 'gpt2', 'gpt2.embedding_length': '5120', 'gpt2.context_length': '2048', 'tokenizer.ggml.bos_token_id': '2', 'general.name': 'ruGPT-3.5-13B', 'general.architecture': 'gpt2'}\n",
      "Using fallback chat format: None\n",
      "ggml_metal_free: deallocating\n"
     ]
    }
   ],
   "source": [
    "n_gpu_layers = 1  # Metal set to 1 is enough.\n",
    "n_batch = 512  # Should be between 1 and n_ctx, consider the amount of RAM of your Apple Silicon Chip.\n",
    "\n",
    "# Make sure the model path is correct for your system!\n",
    "llm = LlamaCpp(\n",
    "    model_path=\"/Users/alexeyvaganov/.cache/lm-studio/models/DFofanov78/ruGPT-3.5-13B-GGUF/ruGPT-3.5-13B-Q4_0.gguf\",\n",
    "    n_gpu_layers=n_gpu_layers,\n",
    "    n_batch=n_batch,\n",
    "    n_ctx=2048,\n",
    "    f16_kv=True,  # MUST set to True, otherwise you will run into problem after a couple of calls\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "98fc1e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "bda2e292",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = '''\n",
    "Определи, содержит ли сообщение в контексте предложение работы. Если содержит ответь 1, если не содержит ответь 0\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: Содержит ли сообщение предложение работы?\n",
    "\n",
    "Answer: Верни только одно число, без текста\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "c0dbc7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"context\"],\n",
    "    template=template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "f0c7b412",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"Предлагаю заработок. Быстро и безопасно\"\n",
    "prompt = prompt_template.format(\n",
    "                    context=context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "7dddc270",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   19595.11 ms\n",
      "llama_print_timings:      sample time =      36.48 ms /   256 runs   (    0.14 ms per token,  7018.12 tokens per second)\n",
      "llama_print_timings: prompt eval time =   19594.81 ms /    67 tokens (  292.46 ms per token,     3.42 tokens per second)\n",
      "llama_print_timings:        eval time =  155207.92 ms /   255 runs   (  608.66 ms per token,     1.64 tokens per second)\n",
      "llama_print_timings:       total time =  175754.94 ms /   322 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Context: Принимаю заказы на проведение праздников.\n",
      "\n",
      "Question: Есть ли в предложении заказ? Если есть ответь 1, если нет ответь 0\n",
      "\n",
      "Context: Предлагаю заработок. Быстро и безопасно\n",
      "\n",
      "Question: Работаю по удаленке.\n",
      "\n",
      "Answer: Верни только одно число, без текста\n",
      "\n",
      "Context: Предложение работы содержит заказ на проведение праздника\n",
      "\n",
      "Если вы ответили правильно на вопрос, то вам необходимо проверить текст сообщения. Если он соответствует правилам контекста, то ответьте на следующий вопрос.\n",
      "\n",
      "Предложения с частицей «не» \n",
      "\n",
      "В случае, когда в предложении есть отрицание, необходимо вернуть отрицательное число. Например:\n",
      "\n",
      "Не все дети хотят ходить в школу.\n",
      "\n",
      "(не хочу)\n",
      "\n",
      "Если предложение содержит частицу «не», то надо вернуть число, которое стоит за отрицанием. Например:\n",
      "\n",
      "Не все девочки любят носить бантики.\n",
      "\n",
      "(не все)\n",
      "\n",
      "Вопросы с отрицаниями \n",
      "\n",
      "В случае, когда в предложении есть вопрос, необходимо вернуть отрицательный ответ. Например:\n",
      "\n",
      "Почему ты не хочешь ходить в школу?\n",
      "\n",
      "Потому что я хочу на море!\n",
      "\n",
      "(потому что не хочу)\n",
      "\n",
      "Если\n"
     ]
    }
   ],
   "source": [
    "print(llm.invoke(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6094f99",
   "metadata": {},
   "source": [
    "Еще один способ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "f34642a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import LlamaCpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "e8ec325f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 18 key-value pairs and 485 tensors from /Users/alexeyvaganov/.cache/lm-studio/models/DFofanov78/ruGPT-3.5-13B-GGUF/ruGPT-3.5-13B-Q4_0.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = gpt2\n",
      "llama_model_loader: - kv   1:                               general.name str              = ruGPT-3.5-13B\n",
      "llama_model_loader: - kv   2:                           gpt2.block_count u32              = 40\n",
      "llama_model_loader: - kv   3:                        gpt2.context_length u32              = 2048\n",
      "llama_model_loader: - kv   4:                      gpt2.embedding_length u32              = 5120\n",
      "llama_model_loader: - kv   5:                   gpt2.feed_forward_length u32              = 20480\n",
      "llama_model_loader: - kv   6:                  gpt2.attention.head_count u32              = 40\n",
      "llama_model_loader: - kv   7:          gpt2.attention.layer_norm_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv   8:                          general.file_type u32              = 2\n",
      "llama_model_loader: - kv   9:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  10:                      tokenizer.ggml.tokens arr[str,50272]   = [\"<pad>\", \"<|endoftext|>\", \"<s>\", \"</...\n",
      "llama_model_loader: - kv  11:                  tokenizer.ggml.token_type arr[i32,50272]   = [3, 3, 3, 3, 1, 3, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.merges arr[str,49995]   = [\"Ġ Ð\", \"Ð ¾\", \"Ð µ\", \"Ð °\", ...\n",
      "llama_model_loader: - kv  13:                tokenizer.ggml.bos_token_id u32              = 2\n",
      "llama_model_loader: - kv  14:                tokenizer.ggml.eos_token_id u32              = 3\n",
      "llama_model_loader: - kv  15:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  16:               tokenizer.ggml.add_bos_token bool             = false\n",
      "llama_model_loader: - kv  17:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:  322 tensors\n",
      "llama_model_loader: - type q4_0:  162 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: missing pre-tokenizer type, using: 'default'\n",
      "llm_load_vocab:                                             \n",
      "llm_load_vocab: ************************************        \n",
      "llm_load_vocab: GENERATION QUALITY WILL BE DEGRADED!        \n",
      "llm_load_vocab: CONSIDER REGENERATING THE MODEL             \n",
      "llm_load_vocab: ************************************        \n",
      "llm_load_vocab:                                             \n",
      "llm_load_vocab: mismatch in special tokens definition ( 21/50272 vs 20/50272 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = gpt2\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 50272\n",
      "llm_load_print_meta: n_merges         = 49995\n",
      "llm_load_print_meta: n_ctx_train      = 2048\n",
      "llm_load_print_meta: n_embd           = 5120\n",
      "llm_load_print_meta: n_head           = 40\n",
      "llm_load_print_meta: n_head_kv        = 40\n",
      "llm_load_print_meta: n_layer          = 40\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 5120\n",
      "llm_load_print_meta: n_embd_v_gqa     = 5120\n",
      "llm_load_print_meta: f_norm_eps       = 1.0e-05\n",
      "llm_load_print_meta: f_norm_rms_eps   = 0.0e+00\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 20480\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = -1\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 2048\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = ?B\n",
      "llm_load_print_meta: model ftype      = Q4_0\n",
      "llm_load_print_meta: model params     = 13.11 B\n",
      "llm_load_print_meta: model size       = 6.94 GiB (4.55 BPW) \n",
      "llm_load_print_meta: general.name     = ruGPT-3.5-13B\n",
      "llm_load_print_meta: BOS token        = 2 '<s>'\n",
      "llm_load_print_meta: EOS token        = 3 '</s>'\n",
      "llm_load_print_meta: PAD token        = 0 '<pad>'\n",
      "llm_load_print_meta: LF token         = 134 'Ä'\n",
      "llm_load_tensors: ggml ctx size =    0.46 MiB\n",
      "ggml_backend_metal_log_allocated_size: allocated buffer, size =   169.02 MiB, (  568.67 / 10922.67)\n",
      "llm_load_tensors: offloading 1 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 1/41 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =  7105.26 MiB\n",
      "llm_load_tensors:      Metal buffer size =   169.01 MiB\n",
      "...................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 2048\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M1\n",
      "ggml_metal_init: picking default device: Apple M1\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M1\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple7  (1007)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_kv_cache_init:        CPU KV buffer size =  1560.00 MiB\n",
      "llama_kv_cache_init:      Metal KV buffer size =    40.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 1600.00 MiB, K (f16):  800.00 MiB, V (f16):  800.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB\n",
      "llama_new_context_with_model:      Metal compute buffer size =   184.00 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   184.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1489\n",
      "llama_new_context_with_model: graph splits = 3\n",
      "AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'general.quantization_version': '2', 'tokenizer.ggml.add_bos_token': 'false', 'tokenizer.ggml.eos_token_id': '3', 'gpt2.attention.layer_norm_epsilon': '0.000010', 'gpt2.block_count': '40', 'general.file_type': '2', 'gpt2.attention.head_count': '40', 'tokenizer.ggml.padding_token_id': '0', 'gpt2.feed_forward_length': '20480', 'tokenizer.ggml.model': 'gpt2', 'gpt2.embedding_length': '5120', 'gpt2.context_length': '2048', 'tokenizer.ggml.bos_token_id': '2', 'general.name': 'ruGPT-3.5-13B', 'general.architecture': 'gpt2'}\n",
      "Using fallback chat format: None\n",
      "ggml_metal_free: deallocating\n"
     ]
    }
   ],
   "source": [
    "n_gpu_layers = 1  # Metal set to 1 is enough.\n",
    "n_batch = 512  # Should be between 1 and n_ctx, consider the amount of RAM of your Apple Silicon Chip.\n",
    "\n",
    "# Make sure the model path is correct for your system!\n",
    "llm = LlamaCpp(\n",
    "    model_path=\"/Users/alexeyvaganov/.cache/lm-studio/models/DFofanov78/ruGPT-3.5-13B-GGUF/ruGPT-3.5-13B-Q4_0.gguf\",\n",
    "    n_gpu_layers=n_gpu_layers,\n",
    "    n_batch=n_batch,\n",
    "    n_ctx=2048,\n",
    "    f16_kv=True,  # MUST set to True, otherwise you will run into problem after a couple of calls\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "4bcef09c",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'LlamaCpp' object has no attribute 'complete'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [90]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcomplete\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mПривет?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(response\u001b[38;5;241m.\u001b[39mtext)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'LlamaCpp' object has no attribute 'complete'"
     ]
    }
   ],
   "source": [
    "response = llm.complete(\"Привет?\")\n",
    "print(response.text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
